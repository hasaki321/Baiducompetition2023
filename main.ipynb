{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "# import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T15:18:21.745196Z",
     "iopub.status.busy": "2023-08-01T15:18:21.744557Z",
     "iopub.status.idle": "2023-08-01T15:18:21.897338Z",
     "shell.execute_reply": "2023-08-01T15:18:21.895242Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.745165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import json\n",
    "path=\"../queries_dataset_merge/\"\n",
    "data_items_train = json.load(open(path+\"dataset_items_train.json\",encoding=\"utf-8\"))\n",
    "data_items_val = json.load(open(path+\"dataset_items_val.json\",encoding=\"utf-8\"))\n",
    "data_items_test = json.load(open(path+\"dataset_items_test.json\",encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据中的每一个样本：图像img、文本caption、对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.898708Z",
     "iopub.status.idle": "2023-08-01T15:18:21.899059Z",
     "shell.execute_reply": "2023-08-01T15:18:21.898894Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.898881Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os \n",
    "import imghdr\n",
    "\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>','')\n",
    "    input_str = input_str.replace('</b>','')\n",
    "    #input_str = unidecode(input_str)  \n",
    "    return input_str\n",
    "    \n",
    "class NewsContextDatasetEmbs(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split):\n",
    "        self.context_data_items_dict = context_data_items_dict\n",
    "        self.queries_root_dir = queries_root_dir\n",
    "        self.idx_to_keys = list(context_data_items_dict.keys())\n",
    "        self.transform =T.Compose([\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "        self.split=split\n",
    "    def __len__(self):\n",
    "        return len(self.context_data_items_dict)   \n",
    "\n",
    "\n",
    "    def load_img_pil(self,image_path):\n",
    "        if imghdr.what(image_path) == 'gif': \n",
    "            try:\n",
    "                with open(image_path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('RGB')\n",
    "            except:\n",
    "                return None \n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def load_imgs_direct_search(self,item_folder_path,direct_dict):   \n",
    "        list_imgs_tensors = []\n",
    "        count = 0   \n",
    "        keys_to_check = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys_to_check:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    image_path = os.path.join(item_folder_path,page['image_path'].split('/')[-1])\n",
    "                    try:\n",
    "                        pil_img = self.load_img_pil(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(image_path)\n",
    "                    if pil_img == None: continue\n",
    "                    transform_img = self.transform(pil_img)\n",
    "                    count = count + 1 \n",
    "                    list_imgs_tensors.append(transform_img)\n",
    "        stacked_tensors = paddle.stack(list_imgs_tensors, axis=0)\n",
    "        return stacked_tensors\n",
    "    def load_captions(self,inv_dict):\n",
    "        captions = ['']\n",
    "        pages_with_captions_keys = ['all_fully_matched_captions','all_partially_matched_captions']\n",
    "        for key1 in pages_with_captions_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        item = page['title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    \n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "                    \n",
    "        pages_with_title_only_keys = ['partially_matched_no_text','fully_matched_no_text']\n",
    "        for key1 in pages_with_title_only_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        title = process_string(page['title'])\n",
    "                        captions.append(title)\n",
    "        return captions\n",
    "\n",
    "    def load_captions_weibo(self,direct_dict):\n",
    "        captions = ['']\n",
    "        keys = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    if 'page_title' in page.keys():\n",
    "                        item = page['page_title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "        #print(captions)\n",
    "        return captions\n",
    "        #加载img文件夹\n",
    "    def load_queries(self,key):\n",
    "        caption = self.context_data_items_dict[key]['caption']\n",
    "        image_path = os.path.join(self.queries_root_dir,self.context_data_items_dict[key]['image_path'])\n",
    "        pil_img = self.load_img_pil(image_path)\n",
    "        transform_img = self.transform(pil_img)\n",
    "        return transform_img, caption\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        #print(self.context_data_items_dict)      \n",
    "        #idx = idx.tolist()               \n",
    "        key = self.idx_to_keys[idx]\n",
    "        #print(key)\n",
    "        item=self.context_data_items_dict.get(str(key))\n",
    "        #print(item)\n",
    "        # 如果为test没有label属性\n",
    "        #print(self.split)\n",
    "        if self.split=='train' or self.split=='val':\n",
    "            label = paddle.to_tensor(int(item['label']))\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'label': label, 'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        else:\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        #print(sample)\n",
    "        #print(len(captions)) \n",
    "        #print(type(imgs))\n",
    "        #print(imgs.size)\n",
    "        #print(imgs.shape)  \n",
    "        return sample,  len(captions), imgs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.900182Z",
     "iopub.status.idle": "2023-08-01T15:18:21.900492Z",
     "shell.execute_reply": "2023-08-01T15:18:21.900356Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.900342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### load Datasets ####\n",
    "train_dataset = NewsContextDatasetEmbs(data_items_train, path,'train')\n",
    "val_dataset = NewsContextDatasetEmbs(data_items_val,path,'val')\n",
    "test_dataset = NewsContextDatasetEmbs(data_items_test,path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.901436Z",
     "iopub.status.idle": "2023-08-01T15:18:21.901764Z",
     "shell.execute_reply": "2023-08-01T15:18:21.901631Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.901617Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 打印数据\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    if step == 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.903225Z",
     "iopub.status.idle": "2023-08-01T15:18:21.903539Z",
     "shell.execute_reply": "2023-08-01T15:18:21.903397Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.903384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle \n",
    "def collate_context_bert_train(batch):\n",
    "    #print(batch)\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    labels = [] \n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        labels.append(sample['label'])\n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0], sample['imgs'].shape[1], sample['imgs'].shape[2], sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        #print(1)\n",
    "        img_batch.append(padded_mem_img)#pad证据图片\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])#[3, 224, 224]\n",
    "        qCap_batch.append(sample['qCap'])     \n",
    "    #print(labels)   \n",
    "    #print(img_batch)\n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    labels = paddle.stack(labels, axis=0) \n",
    "    #print(3)  \n",
    "    return labels, cap_batch, img_batch, qCap_batch, qImg_batch\n",
    "\n",
    "def collate_context_bert_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1],sample['imgs'].shape[2],sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        img_batch.append(padded_mem_img)\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])\n",
    "        qCap_batch.append(sample['qCap'])        \n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    return cap_batch, img_batch, qCap_batch, qImg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.904518Z",
     "iopub.status.idle": "2023-08-01T15:18:21.904831Z",
     "shell.execute_reply": "2023-08-01T15:18:21.904689Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.904676Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn = collate_context_bert_train, return_list=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn = collate_context_bert_train,  return_list=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn = collate_context_bert_test, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.905833Z",
     "iopub.status.idle": "2023-08-01T15:18:21.906150Z",
     "shell.execute_reply": "2023-08-01T15:18:21.906013Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.906000Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-04 00:58:39,863] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-08-04 00:58:39,864] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[32m[2023-08-04 00:58:40,543] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-08-04 00:58:40,544] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap: [['', 'US military investigating leak of emails from Pentagon server - CNN', 'Staff/AFP/Getty Images', 'Pentagon building aerial FILE', 'Lack of transition coordination, Pentagon chaos could leave US vulnerable to national security threats | CNN Politics', \"This picture taken 26 December 2011 shows the Pentagon building in Washington, DC.  The Pentagon, which is the headquarters of the United States Department of Defense (DOD), is the world's largest office building by floor area, with about 6,500,000 sq ft (600,000 m2), of which 3,700,000 sq ft (340,000 m2) are used as offices.  Approximately 23,000 military and civilian employees and about 3,000 non-defense support personnel work in the Pentagon. (Photo credit should read STAFF/AFP via Getty Images)\", 'US military investigating leak of emails from Pentagon server', 'Pentagon documents leak: What we know about the major intelligence leak | CNN Politics', 'Pentagon investigating alleged classified documents circulating on social media of US and NATO intelligence on Ukraine | CNN Politics', '', '4 US service members wounded in helicopter raid that killed IS leader in Syria - ABC News', 'PHOTO: In this file photo from 2019, A helicopter takes off from a US military base at an undisclosed location in eastern Syria, Nov. 11, 2019.', 'US, partner forces capture 2 ISIS militants in eastern Syria, CENTCOM says | Stars and Stripes', 'Syrian Democratic Force soldiers conduct a patrol during a joint operation with U.S. Army soldiers in Syria on May 8, 2021.', 'US conducts helicopter raid in Syria capturing ISIS official - KTVZ', 'US conducts helicopter raid in Syria capturing ISIS official ...', 'U.S. Commandos Capture Six ISIS Officials in Raids in Syria - The New York Times', 'U.S. forces patrolling in northern Syria on Thursday near a rehabilitation center for the children of suspected Islamic State members.', 'A man in combat gear stands to the right of an armored vehicle flying a U.S. flag.'], ['', 'Hailey and Just Bieber s Body Language—Explained', 'Photography, Art, ', 'Attention Required! | Cloudflare', '', 'Justin Bieber and Hailey Baldwin: A Timeline of Their Relationship', 'Justin Bieber and Hailey Baldwin- A Timeline of Their Relationship 113', 'Justin Bieber - Age, Life & Songs', '.css-p0sr5z{color:#595959;padding-right:0.3125rem;font-family:Gilroy,Helvetica,Arial,Sans-serif;font-size:0.75rem;line-height:1;font-weight:800;}', 'Premiere Of YouTube Originals\\' \"Justin Bieber: Seasons\" - Arrivals LOS ANGELES, CALIFORNIA - JANUARY 27: Justin Bieber attends the premiere of YouTube Originals\\' \"Justin Bieber: Seasons\" at Regency Bruin Theatre on January 27, 2020 in Los Angeles, California. (Photo by Jon Kopaloff/Getty Images)', 'Justin Bieber and Hailey Baldwin: A Timeline of Their Relationship', 'Justin Bieber and Hailey Baldwin- A Timeline of Their Relationship 113', 'Justin Bieber - Age, Life & Songs', '.css-p0sr5z{color:#595959;padding-right:0.3125rem;font-family:Gilroy,Helvetica,Arial,Sans-serif;font-size:0.75rem;line-height:1;font-weight:800;}', 'Premiere Of YouTube Originals\\' \"Justin Bieber: Seasons\" - Arrivals LOS ANGELES, CALIFORNIA - JANUARY 27: Justin Bieber attends the premiere of YouTube Originals\\' \"Justin Bieber: Seasons\" at Regency Bruin Theatre on January 27, 2020 in Los Angeles, California. (Photo by Jon Kopaloff/Getty Images)', 'Justin Bieber and Hailey Baldwin: A Timeline of Their Relationship', 'Boat Life! See Justin and Hailey Bieber’s Sweetest Moments Together', 'March 2023', '']] \n",
      " Qcap: [2, 128] \n",
      " ori: ['The US military and Syrian Democratic Forces conducted a helicopter raid in eastern Syria,', 'Justin Bieber with friends spotted out in Stratford Ontario, Canada.March st,']\n",
      "qimg: 2\n",
      "img: 2\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "    tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "    encode_dict_qcap = tokenizer(text = qCap_batch ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "    input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "#     encode_dict_qcap_ = tokenizer(text = cap_batch ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "#     input_ids_qcap_ = paddle.to_tensor(encode_dict_qcap_['input_ids'])\n",
    "    print(\"cap:\",cap_batch,\"\\n\",\"Qcap:\",input_ids_qcap.shape,\"\\n\",\"ori:\",qCap_batch)\n",
    "    print(\"qimg:\",len(qImg_batch))\n",
    "    print(\"img:\",len(img_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型构建\n",
    "**本次赛题为一个NLP与多模态的分类赛题，整体建模采用特征提取、特征交互、预测分类三个阶段**\n",
    "\n",
    "**特征提取：** 对于图像数据，使用ResNet模型进行特征提取、对于文本数据，使用预训练模型Ernie-m多语言模型对中文和英文同时处理，qCap,qImg,（需要验证的标题或图像材料）、caps,imgs（支持验证的文本、图像证据材料）\n",
    "\n",
    "**特征交互**：使用多头自注意力机制，将标题与文本证据材料交互、图像与图像证据材料交互，输出与需要验证的标题和图像的相关证据特征caps_feature、imgs_features\n",
    "\n",
    "**预测分类：** 最后使用全连接层将标题特征、图像特征、相关的文本证据特征、相关的图像证据特征拼接输入到分类器得到最终结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f29e3f853b9445fbeb24189103cdbbcb8364498dc484593a891839994dadbd6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.907532Z",
     "iopub.status.idle": "2023-08-01T15:18:21.907845Z",
     "shell.execute_reply": "2023-08-01T15:18:21.907702Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.907689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode           \n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.resnet = EncoderCNN()\n",
    "        self.classifier1 = nn.Linear(2*(768+2048),1024)\n",
    "        self.classifier2 = nn.Linear(1024,3)\n",
    "        self.attention_text = nn.MultiHeadAttention(768,16)\n",
    "        self.attention_image = nn.MultiHeadAttention(2048,16)\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768,3)\n",
    "        self.resnet.eval()\n",
    "\n",
    "    def forward(self,qCap,qImg,caps,imgs):\n",
    "        self.resnet.eval()\n",
    "        encode_dict_qcap = self.tokenizer(text = qCap ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "        input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "        qcap_feature, _= self.ernie(input_ids_qcap) #(b,length,dim)\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:,0,:].squeeze(1))\n",
    "            return logits\n",
    "        caps_feature = []\n",
    "        for i,caption in enumerate (caps):\n",
    "            encode_dict_cap = self.tokenizer(text = caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "            input_ids_caps = paddle.to_tensor(encode_dict_cap['input_ids'])\n",
    "            cap_feature, _= self.ernie(input_ids_caps) #(b,length,dim)\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature,axis=0) #(b,num,length,dim)\n",
    "        caps_feature = caps_feature.mean(axis=1)#(b,length,dim)\n",
    "        caps_feature = self.attention_text(qcap_feature,caps_feature,caps_feature) #(b,length,dim)\n",
    "        #对于caps，使用Qcap对list中每个cap元素做attention,最后输出caps的shape\n",
    "        imgs_features = []\n",
    "        for img in imgs:\n",
    "            imgs_feature = self.resnet(img) #(length,dim)\n",
    "            imgs_features.append(imgs_feature)\n",
    "        imgs_features = paddle.stack(imgs_features,axis=0) #(b,length,dim)\n",
    "        qImg_features = []\n",
    "        for qImage in qImg:\n",
    "            qImg_feature = self.resnet(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "            qImg_features.append(qImg_feature)\n",
    "        qImg_feature = paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "        imgs_features = self.attention_image(qImg_feature,imgs_features,imgs_features) #(b,1,dim)\n",
    "        # [1, 128, 768] [1, 128, 768] [1, 1, 2048] [1, 1, 2048] origin\n",
    "        feature = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:], qImg_feature.squeeze(1), imgs_features.squeeze(1)], axis=-1) \n",
    "        logits = self.classifier1(feature)\n",
    "        logits = self.classifier2(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-03 00:25:31,434] [    INFO]\u001b[0m - Model config ErnieMConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ernie_m\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:26:00,148] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-m-base were not used when initializing ErnieMModel: ['cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing ErnieMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:26:00,149] [ WARNING]\u001b[0m - Some weights of ErnieMModel were not initialized from the model checkpoint at ernie-m-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:00,243] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:00,244] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:01,129] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:01,130] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = NetWork(\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.910269Z",
     "iopub.status.idle": "2023-08-01T15:18:21.910583Z",
     "shell.execute_reply": "2023-08-01T15:18:21.910439Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.910427Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11184 1118\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "print(num_training_steps,warmup_steps)\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-6, num_training_steps, warmup_steps)\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# 创建保存的文件夹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.911666Z",
     "iopub.status.idle": "2023-08-01T15:18:21.911994Z",
     "shell.execute_reply": "2023-08-01T15:18:21.911836Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.911824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义线下评估 评价指标为acc 线上评估是f1score\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:      \n",
    "        labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "        logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.912957Z",
     "iopub.status.idle": "2023-08-01T15:18:21.913262Z",
     "shell.execute_reply": "2023-08-01T15:18:21.913125Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.913111Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train run start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4bb420a45847fcbd1beb7f46a462e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 128]\n",
      "[2, 128]\n",
      "[2, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义训练\n",
    "from tqdm.auto import tqdm\n",
    "def do_train(model, criterion, metric, val_dataloader,train_dataloader):\n",
    "    print(\"train run start\")\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    best_accuracy=0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(tqdm(train_dataloader), start=1):\n",
    "            labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "            probs = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "            loss = criterion(probs, labels)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1 \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # 每间隔一个epoch 在验证集进行评估\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss,eval_accu=evaluate(model, criterion, metric, val_dataloader)\n",
    "                save_param_path = os.path.join(save_dir+str(epoch), 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                if(best_accuracy<eval_accu):\n",
    "                    best_accuracy=eval_accu\n",
    "                    # 保存模型\n",
    "                    save_param_path = os.path.join(best_dir, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "do_train(model, criterion, metric, val_dataloader,train_dataloader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、模型预测\n",
    "**模型预测前，请重启内核，清空占用的显存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.914554Z",
     "iopub.status.idle": "2023-08-01T15:18:21.914930Z",
     "shell.execute_reply": "2023-08-01T15:18:21.914758Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.914738Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint/model_best.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.915828Z",
     "iopub.status.idle": "2023-08-01T15:18:21.916148Z",
     "shell.execute_reply": "2023-08-01T15:18:21.916010Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.915997Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "count=0\n",
    "for batch in test_dataloader:\n",
    "    count+=1\n",
    "    cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "    logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    label = paddle.argmax(probs, axis=1).numpy()\n",
    "    results += label.tolist()\n",
    "    print(count)\n",
    "print(results[:5])\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.917503Z",
     "iopub.status.idle": "2023-08-01T15:18:21.917866Z",
     "shell.execute_reply": "2023-08-01T15:18:21.917691Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.917678Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "import pandas as pd\n",
    "#id/label\n",
    "#字典中的key值即为csv中的列名\n",
    "id_list=range(len(results))\n",
    "print(id_list)\n",
    "frame = pd.DataFrame({'id':id_list,'label':results})\n",
    "frame.to_csv(\"result.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、后续优化\n",
    "\n",
    "baseline分数只有65分，还有很大的改进地方，大家多多尝试，下面是一些想法\n",
    "\n",
    "参数调优：学习率、优化器以及其他超参数等\n",
    "\n",
    "特征提取：更换预训练权重更大的图像特征提取器or文本特征提取器（Ernie or Bert系列）\n",
    "\n",
    "特征交互：目前使用多头自注意力机制对文本与文本证据交互、图像与图像证据交互，可以尝试文本与图像之间的跨模态交互\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
