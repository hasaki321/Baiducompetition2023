{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "# import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T15:18:21.745196Z",
     "iopub.status.busy": "2023-08-01T15:18:21.744557Z",
     "iopub.status.idle": "2023-08-01T15:18:21.897338Z",
     "shell.execute_reply": "2023-08-01T15:18:21.895242Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.745165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import json\n",
    "path=\"../queries_dataset_merge/\"\n",
    "data_items_train = json.load(open(path+\"dataset_items_train.json\",encoding=\"utf-8\"))\n",
    "data_items_val = json.load(open(path+\"dataset_items_val.json\",encoding=\"utf-8\"))\n",
    "data_items_test = json.load(open(path+\"dataset_items_test.json\",encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据中的每一个样本：图像img、文本caption、对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.898708Z",
     "iopub.status.idle": "2023-08-01T15:18:21.899059Z",
     "shell.execute_reply": "2023-08-01T15:18:21.898894Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.898881Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os \n",
    "import imghdr\n",
    "\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>','')\n",
    "    input_str = input_str.replace('</b>','')\n",
    "    #input_str = unidecode(input_str)  \n",
    "    return input_str\n",
    "    \n",
    "class NewsContextDatasetEmbs(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split):\n",
    "        self.context_data_items_dict = context_data_items_dict\n",
    "        self.queries_root_dir = queries_root_dir\n",
    "        self.idx_to_keys = list(context_data_items_dict.keys())\n",
    "        self.transform =T.Compose([\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "        self.split=split\n",
    "    def __len__(self):\n",
    "        return len(self.context_data_items_dict)   \n",
    "\n",
    "\n",
    "    def load_img_pil(self,image_path):\n",
    "        if imghdr.what(image_path) == 'gif': \n",
    "            try:\n",
    "                with open(image_path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('RGB')\n",
    "            except:\n",
    "                return None \n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def load_imgs_direct_search(self,item_folder_path,direct_dict):   \n",
    "        list_imgs_tensors = []\n",
    "        count = 0   \n",
    "        keys_to_check = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys_to_check:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    image_path = os.path.join(item_folder_path,page['image_path'].split('/')[-1])\n",
    "                    try:\n",
    "                        pil_img = self.load_img_pil(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(image_path)\n",
    "                    if pil_img == None: continue\n",
    "                    transform_img = self.transform(pil_img)\n",
    "                    count = count + 1 \n",
    "                    list_imgs_tensors.append(transform_img)\n",
    "        stacked_tensors = paddle.stack(list_imgs_tensors, axis=0)\n",
    "        return stacked_tensors\n",
    "    def load_captions(self,inv_dict):\n",
    "        captions = ['']\n",
    "        pages_with_captions_keys = ['all_fully_matched_captions','all_partially_matched_captions']\n",
    "        for key1 in pages_with_captions_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        item = page['title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    \n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "                    \n",
    "        pages_with_title_only_keys = ['partially_matched_no_text','fully_matched_no_text']\n",
    "        for key1 in pages_with_title_only_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        title = process_string(page['title'])\n",
    "                        captions.append(title)\n",
    "        return captions\n",
    "\n",
    "    def load_captions_weibo(self,direct_dict):\n",
    "        captions = ['']\n",
    "        keys = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    if 'page_title' in page.keys():\n",
    "                        item = page['page_title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "        #print(captions)\n",
    "        return captions\n",
    "        #加载img文件夹\n",
    "    def load_queries(self,key):\n",
    "        caption = self.context_data_items_dict[key]['caption']\n",
    "        image_path = os.path.join(self.queries_root_dir,self.context_data_items_dict[key]['image_path'])\n",
    "        pil_img = self.load_img_pil(image_path)\n",
    "        transform_img = self.transform(pil_img)\n",
    "        return transform_img, caption\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        #print(self.context_data_items_dict)      \n",
    "        #idx = idx.tolist()               \n",
    "        key = self.idx_to_keys[idx]\n",
    "        #print(key)\n",
    "        item=self.context_data_items_dict.get(str(key))\n",
    "        #print(item)\n",
    "        # 如果为test没有label属性\n",
    "        #print(self.split)\n",
    "        if self.split=='train' or self.split=='val':\n",
    "            label = paddle.to_tensor(int(item['label']))\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'label': label, 'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        else:\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        #print(sample)\n",
    "        #print(len(captions)) \n",
    "        #print(type(imgs))\n",
    "        #print(imgs.size)\n",
    "        #print(imgs.shape)  \n",
    "        return sample,  len(captions), imgs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.900182Z",
     "iopub.status.idle": "2023-08-01T15:18:21.900492Z",
     "shell.execute_reply": "2023-08-01T15:18:21.900356Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.900342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### load Datasets ####\n",
    "train_dataset = NewsContextDatasetEmbs(data_items_train, path,'train')\n",
    "val_dataset = NewsContextDatasetEmbs(data_items_val,path,'val')\n",
    "test_dataset = NewsContextDatasetEmbs(data_items_test,path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.903225Z",
     "iopub.status.idle": "2023-08-01T15:18:21.903539Z",
     "shell.execute_reply": "2023-08-01T15:18:21.903397Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.903384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle \n",
    "def collate_context_bert_train(batch):\n",
    "    #print(batch)\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    labels = [] \n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        labels.append(sample['label'])\n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0], sample['imgs'].shape[1], sample['imgs'].shape[2], sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        #print(1)\n",
    "        img_batch.append(padded_mem_img)#pad证据图片\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])#[3, 224, 224]\n",
    "        qCap_batch.append(sample['qCap'])     \n",
    "    #print(labels)   \n",
    "    #print(img_batch)\n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    labels = paddle.stack(labels, axis=0) \n",
    "    #print(3)  \n",
    "    return labels, cap_batch, img_batch, qCap_batch, qImg_batch\n",
    "\n",
    "def collate_context_bert_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1],sample['imgs'].shape[2],sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        img_batch.append(padded_mem_img)\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])\n",
    "        qCap_batch.append(sample['qCap'])        \n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    return cap_batch, img_batch, qCap_batch, qImg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.904518Z",
     "iopub.status.idle": "2023-08-01T15:18:21.904831Z",
     "shell.execute_reply": "2023-08-01T15:18:21.904689Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.904676Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn = collate_context_bert_train, return_list=True,num_workers=8,prefetch_factor=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn = collate_context_bert_train,  return_list=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn = collate_context_bert_test, return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型构建\n",
    "**本次赛题为一个NLP与多模态的分类赛题，整体建模采用特征提取、特征交互、预测分类三个阶段**\n",
    "\n",
    "**特征提取：** 对于图像数据，使用ResNet模型进行特征提取、对于文本数据，使用预训练模型Ernie-m多语言模型对中文和英文同时处理，qCap,qImg,（需要验证的标题或图像材料）、caps,imgs（支持验证的文本、图像证据材料）\n",
    "\n",
    "**特征交互**：使用多头自注意力机制，将标题与文本证据材料交互、图像与图像证据材料交互，输出与需要验证的标题和图像的相关证据特征caps_feature、imgs_features\n",
    "\n",
    "**预测分类：** 最后使用全连接层将标题特征、图像特征、相关的文本证据特征、相关的图像证据特征拼接输入到分类器得到最终结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f29e3f853b9445fbeb24189103cdbbcb8364498dc484593a891839994dadbd6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.vision import models\n",
    "from paddle import nn\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.resnet_arch = resnet_arch\n",
    "        if resnet_arch == 'resnet101':\n",
    "            convnet = models.resnet101(pretrained=True)\n",
    "            modules = list(convnet.children())[:-2] + [nn.AdaptiveAvgPool2D((1, 1))]\n",
    "            self.convnet = nn.Sequential(*modules)\n",
    "        if resnet_arch == 'densenet169':\n",
    "            convnet = models.densenet169(pretrained=True,num_classes=8*256,with_pool=True)\n",
    "            self.convnet = convnet\n",
    "        \n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.convnet(images)\n",
    "        if self.resnet_arch == 'resnet101':\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "# tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "# convnet = EncoderCNN()\n",
    "# convnet.eval()\n",
    "# ernie.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_qImg_feature(qImg_batch):\n",
    "#     qImg_features = []\n",
    "#     for qImage in qImg_batch:\n",
    "#         qImg_feature = convnet(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "#         qImg_features.append(qImg_feature)\n",
    "#     return paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "\n",
    "# def get_qcap_feature(qCap_batch):\n",
    "#     encode_dict_qcap = tokenizer(text = qCap_batch ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "#     input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "#     qcap_feature, _= ernie(input_ids_qcap) #(b,length,dim)\n",
    "#     return qcap_feature\n",
    "\n",
    "# for step, batch in enumerate(train_dataloader, start=1):\n",
    "#     labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "#     print(img_batch.shape)\n",
    "#     print(qImg_batch.shape)\n",
    "#     print(cap_batch)\n",
    "    \n",
    "#     for img,cap in zip(img_batch,cap_batch):\n",
    "#         qImg_feature = get_qImg_feature(img)\n",
    "#         qImg_feature = qImg_feature.reshape((qImg_feature.shape[0],8,256)).mean(axis=0).unsqueeze(0)\n",
    "#         qcap_feature = get_qcap_feature(cap)\n",
    "#         qcap_feature = qcap_feature.reshape((qcap_feature.shape[0],qcap_feature.shape[1],16,48))\n",
    "#         qcap_feature = nn.Conv2D(128,48,1,1)(qcap_feature)\n",
    "#         qcap_feature = nn.Conv2D(48,8,(1,4),(1,3),(0,1))(qcap_feature)\n",
    "#         qcap_feature = qcap_feature.reshape((qcap_feature.shape[0],8,256)).mean(axis=0).unsqueeze(0)\n",
    "#         print(qcap_feature.shape)\n",
    "#         print(qImg_feature.shape)\n",
    "#         feature1 = nn.MultiHeadAttention(256,16)(qcap_feature,qImg_feature,qImg_feature)\n",
    "#         feature2 = nn.MultiHeadAttention(256,16)(qImg_feature,qcap_feature,qcap_feature)\n",
    "#         print(\"cap:\",qcap_feature.shape)\n",
    "#         print(\"img:\",qImg_feature.shape)\n",
    "#         print(\"fet1:\",feature1.shape)\n",
    "#         print(\"fet2:\",feature2.shape)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.907532Z",
     "iopub.status.idle": "2023-08-01T15:18:21.907845Z",
     "shell.execute_reply": "2023-08-01T15:18:21.907702Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.907689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode           \n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.convnet = EncoderCNN()\n",
    "        if self.mode == \"attempt1\":\n",
    "            self.fnn =nn.Sequential(\n",
    "                nn.Linear(2*(768+2048*3),1024),\n",
    "                nn.Linear(1024,3)\n",
    "            ) \n",
    "        else:\n",
    "            self.fnn =nn.Sequential(\n",
    "                nn.Linear(2*(768+2048),1024),\n",
    "                nn.Linear(1024,3)\n",
    "            ) \n",
    "        self.attention_text = nn.MultiHeadAttention(768,16)\n",
    "        self.attention_image = nn.MultiHeadAttention(2048,16)\n",
    "        \n",
    "        self.attention_align = nn.MultiHeadAttention(256,16)     \n",
    "        self.transform_layer = nn.Sequential(\n",
    "            nn.Conv2D(128,48,1,1),\n",
    "            nn.Conv2D(48,8,(1,4),(1,3),(0,1))\n",
    "        )\n",
    "        \n",
    "#         self.norm = nn.BatchNorm((8,256))\n",
    "        \n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768,3)\n",
    "        self.convnet.eval()\n",
    "        \n",
    "    def alignment(self,qcap_feature,qImg_feature):\n",
    "        if len(qcap_feature.shape) > 4:\n",
    "            qImg_feature = qImg_feature.reshape((qImg_feature.shape[0],8,256)).mean(axis=0).unsqueeze(0)\n",
    "        else:\n",
    "            qImg_feature = qImg_feature.reshape((qImg_feature.shape[0],8,256))\n",
    "#         qImg_feature = self.norm(qImg_feature)\n",
    "        if len(qImg_feature.shape) > 4:\n",
    "            qcap_feature = qcap_feature.reshape((qcap_feature.shape[0],qcap_feature.shape[1],16,48)).mean(axis=0).unsqueeze(0)\n",
    "        else:\n",
    "            qcap_feature = qcap_feature.reshape((qcap_feature.shape[0],qcap_feature.shape[1],16,48))\n",
    "        qcap_feature = self.transform_layer(qcap_feature)\n",
    "        qcap_feature = qcap_feature.reshape((qcap_feature.shape[0],8,256))\n",
    "#         qcap_feature = self.norm(qcap_feature)\n",
    "        \n",
    "        feature1 = self.attention_align(qcap_feature,qImg_feature,qImg_feature)\n",
    "        feature2 = self.attention_align(qImg_feature,qcap_feature,qcap_feature)\n",
    "        return feature1.reshape((feature1.shape[0],2048)),feature2.reshape((feature1.shape[0],2048))\n",
    "    \n",
    "    \n",
    "    def forward(self,qCap,qImg,caps,imgs):\n",
    "        self.convnet.eval()\n",
    "        encode_dict_qcap = self.tokenizer(text = qCap ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "        input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "        qcap_feature, _= self.ernie(input_ids_qcap) #(b,length,dim)\n",
    "        \n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:,0,:].squeeze(1))\n",
    "            return logits\n",
    "        caps_feature = []\n",
    "        for i,caption in enumerate (caps):\n",
    "            encode_dict_cap = self.tokenizer(text = caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "            input_ids_caps = paddle.to_tensor(encode_dict_cap['input_ids'])\n",
    "            cap_feature, _= self.ernie(input_ids_caps) #(b,length,dim)\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature,axis=0) #(b,num,length,dim)\n",
    "        cap_feature = caps_feature.mean(axis=1)#(b,length,dim)\n",
    "        caps_feature = self.attention_text(qcap_feature,cap_feature,cap_feature) #(b,length,dim)\n",
    "        \n",
    "        \n",
    "        imgs_features = []\n",
    "        for img in imgs:\n",
    "            imgs_feature = self.convnet(img) #(length,dim)\n",
    "            imgs_features.append(imgs_feature)\n",
    "        Img_feature = paddle.stack(imgs_features,axis=0) #(b,length,dim)\n",
    "        \n",
    "        \n",
    "        qImg_features = []\n",
    "        for qImage in qImg:\n",
    "            qImg_feature = self.convnet(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "            qImg_features.append(qImg_feature)\n",
    "        qImg_feature = paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        imgs_features = self.attention_image(qImg_feature,Img_feature,Img_feature) #(b,1,dim)\n",
    "        # [b, 128, 768] [b, 128, 768] [b, 1, 2048] [b, 1, 2048] origin\n",
    "        \n",
    "        if self.mode ==\"attempt1\":\n",
    "            qfeature1,qfeature2 = self.alignment(qcap_feature,qImg_feature)\n",
    "            print(\"capbatch:\",cap_feature.shape)\n",
    "            print(\"Imgbatch:\",Img_feature.shape)\n",
    "            feature1,feature2 = self.alignment(cap_feature,Img_feature.mean(axis=1).unsqueeze(1))\n",
    "\n",
    "            feature = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:], qImg_feature.squeeze(1), imgs_features.squeeze(1),qfeature1,qfeature2,feature1,feature2], axis=-1)\n",
    "        else:\n",
    "            feature = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:], qImg_feature.squeeze(1), imgs_features.squeeze(1)], axis=-1) \n",
    "        \n",
    "        logits = self.fnn(feature)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-06 01:32:05,632] [    INFO]\u001b[0m - Model config ErnieMConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ernie_m\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-08-06 01:32:36,858] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-m-base were not used when initializing ErnieMModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias']\n",
      "- This IS expected if you are initializing ErnieMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-08-06 01:32:36,886] [ WARNING]\u001b[0m - Some weights of ErnieMModel were not initialized from the model checkpoint at ernie-m-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-08-06 01:32:36,966] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-08-06 01:32:36,968] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[32m[2023-08-06 01:32:38,082] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-08-06 01:32:38,084] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = NetWork(\"attempt1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.910269Z",
     "iopub.status.idle": "2023-08-01T15:18:21.910583Z",
     "shell.execute_reply": "2023-08-01T15:18:21.910439Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.910427Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5592 559\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "print(num_training_steps,warmup_steps)\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-6, num_training_steps, warmup_steps)\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# 创建保存的文件夹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.911666Z",
     "iopub.status.idle": "2023-08-01T15:18:21.911994Z",
     "shell.execute_reply": "2023-08-01T15:18:21.911836Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.911824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义线下评估 评价指标为acc 线上评估是f1score\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:      \n",
    "        labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "        logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.912957Z",
     "iopub.status.idle": "2023-08-01T15:18:21.913262Z",
     "shell.execute_reply": "2023-08-01T15:18:21.913125Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.913111Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train run start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3ac5c42098469ea8ab2e580f280295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capbatch: [2, 128, 768]\n",
      "Imgbatch: [2, 4, 2048]\n",
      "capbatch: [2, 128, 768]\n",
      "Imgbatch: [2, 2, 2048]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m                     save_param_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(best_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_best.pdparams\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m                     paddle\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_param_path)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m, in \u001b[0;36mdo_train\u001b[1;34m(model, criterion, metric, val_dataloader, train_dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal step \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, epoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, batch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, accu: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, speed: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m step/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;241m%\u001b[39m (global_step, epoch, step, loss, acc,\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m/\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic_train)))\n\u001b[0;32m     27\u001b[0m     tic_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\paddle\\fluid\\wrapped_decorator.py:25\u001b[0m, in \u001b[0;36mwrap_decorator.<locals>.__impl__\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__impl__\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     wrapped_func \u001b[38;5;241m=\u001b[39m decorator_func(func)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\paddle\\fluid\\framework.py:449\u001b[0m, in \u001b[0;36m_dygraph_only_.<locals>.__impl__\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__impl__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_dygraph_mode(), (\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe only support \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in dynamic graph mode, please call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaddle.disable_static()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to enter dynamic graph mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    448\u001b[0m     )\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\paddle\\fluid\\dygraph\\tensor_patch_methods.py:298\u001b[0m, in \u001b[0;36mmonkey_patch_tensor.<locals>.backward\u001b[1;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _grad_scalar:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# When using amp with Fleet DistributedStrategy, we do loss scaling implicitly.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m _grad_scalar\u001b[38;5;241m.\u001b[39mscale(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_profiler_mode():\n\u001b[0;32m    301\u001b[0m     record_event\u001b[38;5;241m.\u001b[39mend()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义训练\n",
    "from tqdm.auto import tqdm\n",
    "def do_train(model, criterion, metric, val_dataloader,train_dataloader):\n",
    "    print(\"train run start\")\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    best_accuracy=0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        bar = tqdm(train_dataloader)\n",
    "        bar.set_description(f\"[Epoch: {epoch}]\")\n",
    "        for step, batch in enumerate(bar, start=1):\n",
    "            labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "            probs = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "            loss = criterion(probs, labels)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1 \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            bar.set_postfix({\"loss\":loss,\"acc:\":acc})\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # 每间隔一个epoch 在验证集进行评估\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss,eval_accu=evaluate(model, criterion, metric, val_dataloader)\n",
    "                save_param_path = os.path.join(save_dir+str(epoch), 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                if(best_accuracy<eval_accu):\n",
    "                    best_accuracy=eval_accu\n",
    "                    # 保存模型\n",
    "                    save_param_path = os.path.join(best_dir, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "do_train(model, criterion, metric, val_dataloader,train_dataloader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、模型预测\n",
    "**模型预测前，请重启内核，清空占用的显存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.914554Z",
     "iopub.status.idle": "2023-08-01T15:18:21.914930Z",
     "shell.execute_reply": "2023-08-01T15:18:21.914758Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.914738Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint/model_best.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.915828Z",
     "iopub.status.idle": "2023-08-01T15:18:21.916148Z",
     "shell.execute_reply": "2023-08-01T15:18:21.916010Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.915997Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "count=0\n",
    "for batch in test_dataloader:\n",
    "    count+=1\n",
    "    cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "    logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    label = paddle.argmax(probs, axis=1).numpy()\n",
    "    results += label.tolist()\n",
    "    print(count)\n",
    "print(results[:5])\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.917503Z",
     "iopub.status.idle": "2023-08-01T15:18:21.917866Z",
     "shell.execute_reply": "2023-08-01T15:18:21.917691Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.917678Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "import pandas as pd\n",
    "#id/label\n",
    "#字典中的key值即为csv中的列名\n",
    "id_list=range(len(results))\n",
    "print(id_list)\n",
    "frame = pd.DataFrame({'id':id_list,'label':results})\n",
    "frame.to_csv(\"result.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、后续优化\n",
    "\n",
    "baseline分数只有65分，还有很大的改进地方，大家多多尝试，下面是一些想法\n",
    "\n",
    "参数调优：学习率、优化器以及其他超参数等\n",
    "\n",
    "特征提取：更换预训练权重更大的图像特征提取器or文本特征提取器（Ernie or Bert系列）\n",
    "\n",
    "特征交互：目前使用多头自注意力机制对文本与文本证据交互、图像与图像证据交互，可以尝试文本与图像之间的跨模态交互\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
