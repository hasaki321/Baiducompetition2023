{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "# import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T15:18:21.745196Z",
     "iopub.status.busy": "2023-08-01T15:18:21.744557Z",
     "iopub.status.idle": "2023-08-01T15:18:21.897338Z",
     "shell.execute_reply": "2023-08-01T15:18:21.895242Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.745165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import json\n",
    "path=\"../queries_dataset_merge/\"\n",
    "data_items_train = json.load(open(path+\"dataset_items_train.json\",encoding=\"utf-8\"))\n",
    "data_items_val = json.load(open(path+\"dataset_items_val.json\",encoding=\"utf-8\"))\n",
    "data_items_test = json.load(open(path+\"dataset_items_test.json\",encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据中的每一个样本：图像img、文本caption、对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.898708Z",
     "iopub.status.idle": "2023-08-01T15:18:21.899059Z",
     "shell.execute_reply": "2023-08-01T15:18:21.898894Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.898881Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os \n",
    "import imghdr\n",
    "\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>','')\n",
    "    input_str = input_str.replace('</b>','')\n",
    "    #input_str = unidecode(input_str)  \n",
    "    return input_str\n",
    "    \n",
    "class NewsContextDatasetEmbs(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split):\n",
    "        self.context_data_items_dict = context_data_items_dict\n",
    "        self.queries_root_dir = queries_root_dir\n",
    "        self.idx_to_keys = list(context_data_items_dict.keys())\n",
    "        self.transform =T.Compose([\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "        self.split=split\n",
    "    def __len__(self):\n",
    "        return len(self.context_data_items_dict)   \n",
    "\n",
    "\n",
    "    def load_img_pil(self,image_path):\n",
    "        if imghdr.what(image_path) == 'gif': \n",
    "            try:\n",
    "                with open(image_path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('RGB')\n",
    "            except:\n",
    "                return None \n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def load_imgs_direct_search(self,item_folder_path,direct_dict):   \n",
    "        list_imgs_tensors = []\n",
    "        count = 0   \n",
    "        keys_to_check = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys_to_check:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    image_path = os.path.join(item_folder_path,page['image_path'].split('/')[-1])\n",
    "                    try:\n",
    "                        pil_img = self.load_img_pil(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(image_path)\n",
    "                    if pil_img == None: continue\n",
    "                    transform_img = self.transform(pil_img)\n",
    "                    count = count + 1 \n",
    "                    list_imgs_tensors.append(transform_img)\n",
    "        stacked_tensors = paddle.stack(list_imgs_tensors, axis=0)\n",
    "        return stacked_tensors\n",
    "    def load_captions(self,inv_dict):\n",
    "        captions = ['']\n",
    "        pages_with_captions_keys = ['all_fully_matched_captions','all_partially_matched_captions']\n",
    "        for key1 in pages_with_captions_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        item = page['title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    \n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "                    \n",
    "        pages_with_title_only_keys = ['partially_matched_no_text','fully_matched_no_text']\n",
    "        for key1 in pages_with_title_only_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        title = process_string(page['title'])\n",
    "                        captions.append(title)\n",
    "        return captions\n",
    "\n",
    "    def load_captions_weibo(self,direct_dict):\n",
    "        captions = ['']\n",
    "        keys = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    if 'page_title' in page.keys():\n",
    "                        item = page['page_title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "        #print(captions)\n",
    "        return captions\n",
    "        #加载img文件夹\n",
    "    def load_queries(self,key):\n",
    "        caption = self.context_data_items_dict[key]['caption']\n",
    "        image_path = os.path.join(self.queries_root_dir,self.context_data_items_dict[key]['image_path'])\n",
    "        pil_img = self.load_img_pil(image_path)\n",
    "        transform_img = self.transform(pil_img)\n",
    "        return transform_img, caption\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        #print(self.context_data_items_dict)      \n",
    "        #idx = idx.tolist()               \n",
    "        key = self.idx_to_keys[idx]\n",
    "        #print(key)\n",
    "        item=self.context_data_items_dict.get(str(key))\n",
    "        #print(item)\n",
    "        # 如果为test没有label属性\n",
    "        #print(self.split)\n",
    "        if self.split=='train' or self.split=='val':\n",
    "            label = paddle.to_tensor(int(item['label']))\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'label': label, 'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        else:\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json'),encoding=\"utf-8\"))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json'),encoding=\"utf-8\"))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        #print(sample)\n",
    "        #print(len(captions)) \n",
    "        #print(type(imgs))\n",
    "        #print(imgs.size)\n",
    "        #print(imgs.shape)  \n",
    "        return sample,  len(captions), imgs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.900182Z",
     "iopub.status.idle": "2023-08-01T15:18:21.900492Z",
     "shell.execute_reply": "2023-08-01T15:18:21.900356Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.900342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### load Datasets ####\n",
    "train_dataset = NewsContextDatasetEmbs(data_items_train, path,'train')\n",
    "val_dataset = NewsContextDatasetEmbs(data_items_val,path,'val')\n",
    "test_dataset = NewsContextDatasetEmbs(data_items_test,path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.901436Z",
     "iopub.status.idle": "2023-08-01T15:18:21.901764Z",
     "shell.execute_reply": "2023-08-01T15:18:21.901631Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.901617Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    print(len(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.903225Z",
     "iopub.status.idle": "2023-08-01T15:18:21.903539Z",
     "shell.execute_reply": "2023-08-01T15:18:21.903397Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.903384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle \n",
    "def collate_context_bert_train(batch):\n",
    "    #print(batch)\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    labels = [] \n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        labels.append(sample['label'])\n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0], sample['imgs'].shape[1], sample['imgs'].shape[2], sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        #print(1)\n",
    "        img_batch.append(padded_mem_img)#pad证据图片\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])#[3, 224, 224]\n",
    "        qCap_batch.append(sample['qCap'])     \n",
    "    #print(labels)   \n",
    "    #print(img_batch)\n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    labels = paddle.stack(labels, axis=0) \n",
    "    #print(3)  \n",
    "    return labels, cap_batch, img_batch, qCap_batch, qImg_batch\n",
    "\n",
    "def collate_context_bert_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1],sample['imgs'].shape[2],sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        img_batch.append(padded_mem_img)\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])\n",
    "        qCap_batch.append(sample['qCap'])        \n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    return cap_batch, img_batch, qCap_batch, qImg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.904518Z",
     "iopub.status.idle": "2023-08-01T15:18:21.904831Z",
     "shell.execute_reply": "2023-08-01T15:18:21.904689Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.904676Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn = collate_context_bert_train, return_list=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn = collate_context_bert_train,  return_list=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn = collate_context_bert_test, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.905833Z",
     "iopub.status.idle": "2023-08-01T15:18:21.906150Z",
     "shell.execute_reply": "2023-08-01T15:18:21.906013Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.906000Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[2], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [0, 1]), [['', '国防部：军演是对美台勾连的震慑 美方与承诺背道而驰_军事频道_中华网', '国防部：军演是对美台勾连的震慑', '美议员窜访台湾，我东北战区台海方向战备警巡回敬_军事_中华网', '美议员窜访台湾，我东北战区台海方向战备警巡回敬', '国防部：东部战区在台岛周边海空域针对性演练就是针对美台勾连的严正震慑', '点击查看大图', '美媒称美军高层近日多次给中国军队领导打电话被拒，国防部回应', '点击查看大图', '美媒称美军高层近日多次给中国军队领导打电话被拒，国防部回应', '点击查看大图', '国防部：东部战区在台岛周边海空域针对性演练就是针对美台勾连的严正震慑', '点击查看大图', '\\\\xb9\\\\xfa\\\\xb7\\\\xc0\\\\xb2\\\\xbf\\\\xd0\\\\xc2\\\\xceŷ\\\\xa2\\\\xd1\\\\xd4\\\\xc8\\\\xcb̷\\\\xbf˷Ǿ\\\\xcd\\\\xce\\\\xd2ϵ\\\\xc1о\\\\xfc\\\\xca·\\\\xb4\\\\xd6\\\\xc6\\\\xd0ж\\\\xaf\\\\xb7\\\\xa2\\\\xb1\\\\xed̸\\\\xbb\\\\xb0--\\\\xbe\\\\xfc\\\\xca\\\\xc2--\\\\xc8\\\\xcb\\\\xc3\\\\xf1\\\\xcd\\\\xf8', '', '军事专家解读解放军台海演习：首次组织航母编队威慑演练，核潜艇参演_解放军台海演习画面：导弹直耸擎天_台媒：台军升级警戒 监控东风导弹_解放军“锁台”演习今天开始', '国防部：军演是针对美台勾连的严正震慑_京报网', '解放军台海演习首次组织航母编队威慑演练，核潜艇参演 - 知乎', '解放军台海演习首次组织航母编队威慑演练，核潜艇参演 - 知乎', '解放军台海演习首次组织航母编队威慑演练，核潜艇参演 - 知乎', '实弹射击、导弹发射画面公布，命中这些区域！国防部：中国军队说话是算数的！_解放军台海演习航母、核潜艇参演_中国台湾地区_战区', '国防部新闻发言人谭克非就我系列军事反制行动发表谈话_国内聚焦_天下_新闻中心_台海网', '全部精准命中！震撼画面公布！国防部：中国军队说话是算数的！_中方_美方_挑衅', '“中国说话是算数的!”远火跨海、导弹越台意味什么? -6park.com'], ['', '', 'Twenty Years After 9/11: The Fight for Supremacy in Northwest Syria and the Implications for Global Jihad – Combating Terrorism Center at West Point', 'Syrian fighters attend a mock battle in anticipation of an attack by the regime on Idlib province and the surrounding countryside, during a graduation of new Hayat Tahrir al-Sham (HTS) members at a camp in the countryside of the northern Idlib province on August 14, 2018. (Omar Haj Kadour/AFP via Getty Images)', '228 Kud Photos and Premium High Res Pictures - Getty Images', 'Kashmiri protesters hold Pakistan flag during a protest against the proposed Sainik and Pandit colonies and killing of a youth in Kud Shootout, at...', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']], Tensor(shape=[2, 9, 3, 224, 224], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [[[[[-0.83354741, -1.31304061, -0.43967795, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898],\n",
      "           [-0.14855716, -1.48428810, -1.22741675, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898],\n",
      "           [ 0.75905472, -1.14179289, -1.24454141, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898],\n",
      "           ...,\n",
      "           [-1.29591572, -0.42255321,  0.09118938, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898],\n",
      "           [ 0.19393790,  0.69055575,  1.01592600, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898],\n",
      "           [ 1.46116984,  1.61529267,  1.52966881, ...,  2.21465898,\n",
      "             2.21465898,  2.21465898]],\n",
      "\n",
      "          [[ 0.95798326,  1.08053231,  1.57072854, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486],\n",
      "           [ 1.32563055,  1.02801120,  1.11554623, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486],\n",
      "           [ 1.92086864,  0.99299723,  0.97549027, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486],\n",
      "           ...,\n",
      "           [-0.05742282,  0.10014019,  0.25770321, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486],\n",
      "           [ 0.85294122,  0.97549027,  1.20308125, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486],\n",
      "           [ 1.58823562,  1.72829151,  1.64075661, ...,  2.41106486,\n",
      "             2.41106486,  2.41106486]],\n",
      "\n",
      "          [[ 1.89054513,  2.25655818,  2.34370399, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 1.99512029,  2.30884552,  2.32627511, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.39599180,  2.16941214,  2.20427060, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 0.56592613,  0.37420499,  0.37420499, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 1.28052306,  1.08880186,  1.35023987, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 1.80339909,  1.64653635,  1.68139470, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010]]],\n",
      "\n",
      "\n",
      "         [[[-1.19316721, -1.15891767, -1.22741675, ..., -1.33016539,\n",
      "            -1.29591572, -1.27879095],\n",
      "           [-1.19316721, -0.95342064, -0.90204638, ..., -1.31304061,\n",
      "            -1.41578913, -1.15891767],\n",
      "           [-1.15891767, -0.67942464, -0.55955136, ..., -1.36441481,\n",
      "            -1.60416138, -1.14179289],\n",
      "           ...,\n",
      "           [-1.36441481, -1.38153958, -1.38153958, ..., -1.38153958,\n",
      "            -1.38153958, -1.39866436],\n",
      "           [-1.36441481, -1.38153958, -1.38153958, ..., -1.38153958,\n",
      "            -1.38153958, -1.39866436],\n",
      "           [-1.36441481, -1.38153958, -1.38153958, ..., -1.38153958,\n",
      "            -1.38153958, -1.39866436]],\n",
      "\n",
      "          [[-0.44257697, -0.37254897, -0.37254897, ..., -0.56512600,\n",
      "            -0.60013998, -0.54761899],\n",
      "           [-0.40756297, -0.02240882,  0.11764719, ..., -0.33753484,\n",
      "            -0.51260501, -0.17997183],\n",
      "           [-0.30252084,  0.39775920,  0.60784322, ..., -0.21498583,\n",
      "            -0.51260501, -0.00490182],\n",
      "           ...,\n",
      "           [-0.91526604, -0.89775902, -0.89775902, ..., -0.96778703,\n",
      "            -0.96778703, -0.98529404],\n",
      "           [-0.91526604, -0.89775902, -0.89775902, ..., -0.96778703,\n",
      "            -0.96778703, -0.98529404],\n",
      "           [-0.91526604, -0.89775902, -0.89775902, ..., -0.96778703,\n",
      "            -0.96778703, -0.98529404]],\n",
      "\n",
      "          [[ 0.61821371,  0.68793046,  0.65307206, ...,  0.42649257,\n",
      "             0.39163420,  0.42649257],\n",
      "           [ 0.61821371,  0.93193918,  1.03651428, ...,  0.53106773,\n",
      "             0.39163420,  0.67050129],\n",
      "           [ 0.68793046,  1.26309383,  1.43738580, ...,  0.56592613,\n",
      "             0.28705901,  0.75764722],\n",
      "           ...,\n",
      "           [ 0.21734224,  0.19991305,  0.19991305, ...,  0.13019629,\n",
      "             0.13019629,  0.09533790],\n",
      "           [ 0.21734224,  0.19991305,  0.19991305, ...,  0.13019629,\n",
      "             0.13019629,  0.09533790],\n",
      "           [ 0.21734224,  0.19991305,  0.19991305, ...,  0.13019629,\n",
      "             0.13019629,  0.09533790]]],\n",
      "\n",
      "\n",
      "         [[[-0.26843041, -0.31980470, -0.49105233, ..., -0.11430765,\n",
      "            -0.11430765, -0.13143240],\n",
      "           [-0.33692944, -0.30267993, -0.31980470, ..., -0.13143240,\n",
      "            -0.13143240, -0.13143240],\n",
      "           [-0.38830370, -0.28555518, -0.25130567, ..., -0.14855716,\n",
      "            -0.14855716, -0.14855716],\n",
      "           ...,\n",
      "           [-0.25130567, -0.01155914,  0.03981512, ...,  1.10154974,\n",
      "            -0.52530187, -0.88492167],\n",
      "           [ 0.00556562,  0.24531215,  0.29668641, ...,  1.30704713,\n",
      "             0.22818740, -0.83354741],\n",
      "           [ 0.74193001,  0.72480524,  0.74193001, ...,  1.47829461,\n",
      "             0.96455175, -0.62805039]],\n",
      "\n",
      "          [[ 1.04551828,  0.94047624,  0.67787123, ...,  1.13305330,\n",
      "             1.13305330,  1.11554623],\n",
      "           [ 0.97549027,  1.01050425,  0.92296922, ...,  1.11554623,\n",
      "             1.11554623,  1.11554623],\n",
      "           [ 0.94047624,  1.04551828,  1.04551828, ...,  1.09803927,\n",
      "             1.09803927,  1.09803927],\n",
      "           ...,\n",
      "           [-0.12745082, -0.00490182,  0.03011219, ...,  1.08053231,\n",
      "            -0.51260501, -0.95028001],\n",
      "           [ 0.08263319,  0.22268920,  0.25770321, ...,  1.22058833,\n",
      "             0.17016819, -0.96778703],\n",
      "           [ 0.73039222,  0.67787123,  0.66036421, ...,  1.34313750,\n",
      "             0.83543426, -0.84523803]],\n",
      "\n",
      "          [[ 2.08226609,  1.99512029,  1.68139470, ...,  2.15198302,\n",
      "             2.15198302,  2.15198302],\n",
      "           [ 2.09969544,  2.15198302,  2.04740787, ...,  2.13455367,\n",
      "             2.13455367,  2.13455367],\n",
      "           [ 2.09969544,  2.27398729,  2.23912883, ...,  2.11712456,\n",
      "             2.11712456,  2.13455367],\n",
      "           ...,\n",
      "           [-0.30553368, -0.20095852, -0.20095852, ...,  0.86222237,\n",
      "            -0.70640510, -1.12470579],\n",
      "           [-0.09638323,  0.02562112,  0.04305032, ...,  0.93193918,\n",
      "            -0.11381242, -1.19442248],\n",
      "           [ 0.54849690,  0.46135095,  0.46135095, ...,  0.98422676,\n",
      "             0.51363856, -1.12470579]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.04341149,  1.95778763,  2.16328478, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.18040943,  2.19753432,  2.18040943, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.23178363,  2.23178363, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           ...,\n",
      "           [ 2.00916195,  0.50218344,  0.53643292, ...,  0.93030226,\n",
      "             1.10154974,  1.94066298],\n",
      "           [ 1.94066298,  1.56391835,  1.47829461, ...,  1.22142327,\n",
      "             2.09478569,  2.06053615],\n",
      "           [ 2.14615989,  2.16328478,  2.16328478, ...,  2.16328478,\n",
      "             2.18040943,  2.19753432]],\n",
      "\n",
      "          [[ 2.21848750,  2.13095260,  2.34103680, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.35854340,  2.37605071,  2.35854340, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.41106486,  2.41106486, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           ...,\n",
      "           [ 2.14845967,  0.59033620,  0.59033620, ...,  1.06302524,\n",
      "             1.16806722,  2.30602264],\n",
      "           [ 2.09593868,  1.71078455,  1.60574257, ...,  1.37815154,\n",
      "             2.25350142,  2.35854340],\n",
      "           [ 2.32352948,  2.34103680,  2.32352948, ...,  2.34103680,\n",
      "             2.35854340,  2.37605071]],\n",
      "\n",
      "          [[ 2.43085027,  2.34370399,  2.55285430, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.57028365,  2.58771276,  2.57028365, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.62257099,  2.62257099, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 2.25655818,  0.68793046,  1.17594790, ...,  1.24566460,\n",
      "             1.26309383,  2.51799583],\n",
      "           [ 2.23912883,  1.87311590,  1.96026182, ...,  1.54196119,\n",
      "             2.43085027,  2.60514212],\n",
      "           [ 2.55285430,  2.57028365,  2.58771276, ...,  2.51799583,\n",
      "             2.55285430,  2.60514212]]],\n",
      "\n",
      "\n",
      "         [[[ 2.23178363,  2.21465898,  2.14615989, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.23178363,  2.23178363,  2.19753432, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.23178363, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           ...,\n",
      "           [ 2.18040943,  2.19753432,  2.21465898, ...,  2.23178363,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.16328478,  2.21465898,  2.21465898, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 1.39267087,  2.02628660,  1.94066298, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828]],\n",
      "\n",
      "          [[ 2.37605071,  2.35854340,  2.37605071, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.39355755,  2.39355755,  2.41106486, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           ...,\n",
      "           [ 2.35854340,  2.32352948,  2.34103680, ...,  2.41106486,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.32352948,  2.34103680,  2.32352948, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 1.51820755,  2.09593868,  2.09593868, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146]],\n",
      "\n",
      "          [[ 2.51799583,  2.53542542,  2.60514212, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.55285430,  2.57028365,  2.62257099, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.60514212,  2.62257099,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 2.57028365,  2.58771276,  2.43085027, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.51799583,  2.58771276,  2.43085027, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 1.71625316,  2.36113334,  2.27398729, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010]]],\n",
      "\n",
      "\n",
      "         [[[ 2.06053615,  1.70091641,  1.76941538, ...,  1.22142327,\n",
      "             1.23854804,  1.22142327],\n",
      "           [ 2.12903523,  1.47829461,  1.44404507, ...,  1.25567281,\n",
      "             1.23854804,  1.22142327],\n",
      "           [ 2.18040943,  1.51254416,  1.30704713, ...,  1.06730032,\n",
      "             1.15292406,  1.23854804],\n",
      "           ...,\n",
      "           [ 1.27279758,  1.27279758,  1.27279758, ...,  1.27279758,\n",
      "             1.27279758,  1.27279758],\n",
      "           [ 1.27279758,  1.27279758,  1.27279758, ...,  1.27279758,\n",
      "             1.27279758,  1.27279758],\n",
      "           [ 1.27279758,  1.27279758,  1.27279758, ...,  1.27279758,\n",
      "             1.27279758,  1.27279758]],\n",
      "\n",
      "          [[ 2.28851557,  1.92086864,  1.93837559, ...,  1.99089658,\n",
      "             2.00840354,  1.99089658],\n",
      "           [ 2.27100873,  1.57072854,  1.51820755, ...,  2.02591062,\n",
      "             2.00840354,  1.99089658],\n",
      "           [ 2.28851557,  1.53571451,  1.36064458, ...,  1.83333361,\n",
      "             1.92086864,  2.00840354],\n",
      "           ...,\n",
      "           [ 1.99089658,  1.99089658,  1.99089658, ...,  1.99089658,\n",
      "             1.99089658,  1.99089658],\n",
      "           [ 1.99089658,  1.99089658,  1.99089658, ...,  1.99089658,\n",
      "             1.99089658,  1.99089658],\n",
      "           [ 1.99089658,  1.99089658,  1.99089658, ...,  1.99089658,\n",
      "             1.99089658,  1.99089658]],\n",
      "\n",
      "          [[ 1.94283271,  1.64653635,  1.64653635, ...,  2.53542542,\n",
      "             2.55285430,  2.53542542],\n",
      "           [ 1.90797424,  1.40252745,  1.29795229, ...,  2.58771276,\n",
      "             2.57028365,  2.55285430],\n",
      "           [ 1.92540348,  1.43738580,  1.21080625, ...,  2.39599180,\n",
      "             2.48313761,  2.57028365],\n",
      "           ...,\n",
      "           [ 2.43085027,  2.43085027,  2.43085027, ...,  2.43085027,\n",
      "             2.43085027,  2.43085027],\n",
      "           [ 2.43085027,  2.43085027,  2.43085027, ...,  2.43085027,\n",
      "             2.43085027,  2.43085027],\n",
      "           [ 2.43085027,  2.43085027,  2.43085027, ...,  2.43085027,\n",
      "             2.43085027,  2.43085027]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647],\n",
      "           [-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647],\n",
      "           [-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647],\n",
      "           ...,\n",
      "           [-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647],\n",
      "           [-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647],\n",
      "           [-1.94665647, -1.94665647, -1.94665647, ..., -1.94665647,\n",
      "            -1.94665647, -1.94665647]],\n",
      "\n",
      "          [[-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700],\n",
      "           [-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700],\n",
      "           [-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700],\n",
      "           ...,\n",
      "           [-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700],\n",
      "           [-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700],\n",
      "           [-1.31792700, -1.31792700, -1.31792700, ..., -1.31792700,\n",
      "            -1.31792700, -1.31792700]],\n",
      "\n",
      "          [[-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191],\n",
      "           [-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191],\n",
      "           [-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191],\n",
      "           ...,\n",
      "           [-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191],\n",
      "           [-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191],\n",
      "           [-0.77612191, -0.77612191, -0.77612191, ..., -0.77612191,\n",
      "            -0.77612191, -0.77612191]]],\n",
      "\n",
      "\n",
      "         [[[-1.03904438, -1.07329392,  0.55355769, ..., -1.14179289,\n",
      "            -1.14179289, -1.14179289],\n",
      "           [ 0.09118938, -1.17604244, -0.31980470, ..., -1.10754347,\n",
      "            -1.10754347, -1.10754347],\n",
      "           [ 1.40979564,  0.75905472,  0.98167652, ..., -1.07329392,\n",
      "            -1.07329392, -1.09041870],\n",
      "           ...,\n",
      "           [-1.80965841, -1.79253364, -1.79253364, ..., -1.72403467,\n",
      "            -1.72403467, -1.72403467],\n",
      "           [-1.80965841, -1.79253364, -1.79253364, ..., -1.72403467,\n",
      "            -1.72403467, -1.72403467],\n",
      "           [-1.80965841, -1.79253364, -1.79253364, ..., -1.72403467,\n",
      "            -1.72403467, -1.72403467]],\n",
      "\n",
      "          [[-0.93277299, -0.96778703,  0.69537824, ..., -1.16036403,\n",
      "            -1.16036403, -1.16036403],\n",
      "           [ 0.22268920, -1.07282901, -0.21498583, ..., -1.12535000,\n",
      "            -1.12535000, -1.12535000],\n",
      "           [ 1.55322158,  0.88795525,  1.11554623, ..., -1.09033608,\n",
      "            -1.09033608, -1.09033608],\n",
      "           ...,\n",
      "           [-1.72058821, -1.70308125, -1.70308125, ..., -1.63305318,\n",
      "            -1.63305318, -1.63305318],\n",
      "           [-1.72058821, -1.70308125, -1.70308125, ..., -1.63305318,\n",
      "            -1.63305318, -1.63305318],\n",
      "           [-1.72058821, -1.70308125, -1.70308125, ..., -1.63305318,\n",
      "            -1.63305318, -1.63305318]],\n",
      "\n",
      "          [[-0.25324610, -0.30553368,  1.36766899, ...,  0.00819193,\n",
      "             0.00819193,  0.00819193],\n",
      "           [ 0.87965161, -0.35782126,  0.49620935, ...,  0.00819193,\n",
      "             0.00819193,  0.00819193],\n",
      "           [ 2.13455367,  1.48967338,  1.85568666, ...,  0.00819193,\n",
      "             0.00819193, -0.00923726],\n",
      "           ...,\n",
      "           [-1.49071896, -1.47328973, -1.47328973, ..., -1.40357304,\n",
      "            -1.40357304, -1.40357304],\n",
      "           [-1.49071896, -1.47328973, -1.47328973, ..., -1.40357304,\n",
      "            -1.40357304, -1.40357304],\n",
      "           [-1.49071896, -1.47328973, -1.47328973, ..., -1.40357304,\n",
      "            -1.40357304, -1.40357304]]],\n",
      "\n",
      "\n",
      "         [[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]]],\n",
      "\n",
      "\n",
      "         [[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]]],\n",
      "\n",
      "\n",
      "         [[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]],\n",
      "\n",
      "          [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           ...,\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ],\n",
      "           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "             0.        ,  0.        ]]]]]), ['中国军队说话是算数的8月2日美众议院议长佩洛西窜访中国台湾地区', 'Jamp'], Tensor(shape=[2, 3, 224, 224], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [[[[ 1.95778763,  0.07406463,  0.91317749, ...,  2.24890828,\n",
      "            2.24890828,  2.24890828],\n",
      "          [ 0.96455175, -0.26843041,  0.36518541, ...,  2.24890828,\n",
      "            2.24890828,  2.24890828],\n",
      "          [ 1.71804118,  1.59816790,  1.75229061, ...,  2.24890828,\n",
      "            2.24890828,  2.24890828],\n",
      "          ...,\n",
      "          [ 2.23178363,  2.21465898,  2.21465898, ...,  2.23178363,\n",
      "            2.19753432,  2.23178363],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "            2.24890828,  2.24890828],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "            2.24890828,  2.24890828]],\n",
      "\n",
      "         [[ 2.02591062,  0.27521020,  1.18557429, ...,  2.42857146,\n",
      "            2.42857146,  2.42857146],\n",
      "          [ 0.99299723, -0.07492982,  0.60784322, ...,  2.42857146,\n",
      "            2.42857146,  2.42857146],\n",
      "          [ 1.90336156,  1.81582654,  1.97338963, ...,  2.42857146,\n",
      "            2.42857146,  2.42857146],\n",
      "          ...,\n",
      "          [ 2.41106486,  2.41106486,  2.39355755, ...,  2.41106486,\n",
      "            2.41106486,  2.41106486],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "            2.42857146,  2.42857146],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "            2.42857146,  2.42857146]],\n",
      "\n",
      "         [[ 1.83825755,  0.47878015,  1.41995656, ...,  2.64000010,\n",
      "            2.64000010,  2.64000010],\n",
      "          [ 0.80993479,  0.13019629,  0.75764722, ...,  2.64000010,\n",
      "            2.64000010,  2.64000010],\n",
      "          [ 1.92540348,  1.99512029,  2.18684125, ...,  2.64000010,\n",
      "            2.64000010,  2.64000010],\n",
      "          ...,\n",
      "          [ 2.62257099,  2.60514212,  2.60514212, ...,  2.58771276,\n",
      "            2.60514212,  2.58771276],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "            2.64000010,  2.64000010],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "            2.64000010,  2.64000010]]],\n",
      "\n",
      "\n",
      "        [[[-0.69654936, -0.69654936, -0.69654936, ...,  2.23178363,\n",
      "            2.23178363,  2.23178363],\n",
      "          [-0.76504838, -0.76504838, -0.76504838, ...,  2.23178363,\n",
      "            2.23178363,  2.23178363],\n",
      "          [-0.86779690, -0.86779690, -0.86779690, ...,  2.21465898,\n",
      "            2.23178363,  2.23178363],\n",
      "          ...,\n",
      "          [ 0.34806067,  0.36518541,  0.34806067, ...,  1.63241732,\n",
      "            1.44404507,  1.25567281],\n",
      "          [ 0.38231018,  0.38231018,  0.34806067, ...,  1.32417178,\n",
      "            0.86180323,  0.34806067],\n",
      "          [ 0.41655967,  0.41655967,  0.38231018, ...,  0.57068247,\n",
      "            0.02269037, -0.45680270]],\n",
      "\n",
      "         [[-0.84523803, -0.84523803, -0.84523803, ...,  2.41106486,\n",
      "            2.41106486,  2.41106486],\n",
      "          [-0.91526604, -0.91526604, -0.91526604, ...,  2.41106486,\n",
      "            2.41106486,  2.41106486],\n",
      "          [-1.02030802, -1.02030802, -1.02030802, ...,  2.41106486,\n",
      "            2.41106486,  2.41106486],\n",
      "          ...,\n",
      "          [ 0.43277320,  0.45028022,  0.45028022, ...,  1.64075661,\n",
      "            1.44817960,  1.25560224],\n",
      "          [ 0.46778721,  0.46778721,  0.43277320, ...,  1.34313750,\n",
      "            0.85294122,  0.32773119],\n",
      "          [ 0.50280124,  0.50280124,  0.46778721, ...,  0.57282925,\n",
      "            0.01260518, -0.47759098]],\n",
      "\n",
      "         [[-1.15956414, -1.15956414, -1.15956414, ...,  2.46570849,\n",
      "            2.44827914,  2.44827914],\n",
      "          [-1.22928095, -1.22928095, -1.22928095, ...,  2.46570849,\n",
      "            2.44827914,  2.44827914],\n",
      "          [-1.33385611, -1.33385611, -1.33385611, ...,  2.44827914,\n",
      "            2.44827914,  2.41342068],\n",
      "          ...,\n",
      "          [ 0.33934662,  0.33934662,  0.32191741, ...,  1.57681954,\n",
      "            1.40252745,  1.21080625],\n",
      "          [ 0.39163420,  0.35677579,  0.32191741, ...,  1.24566460,\n",
      "            0.77507645,  0.23477145],\n",
      "          [ 0.42649257,  0.40906337,  0.37420499, ...,  0.44392177,\n",
      "           -0.09638323, -0.60182995]]]])]\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型构建\n",
    "**本次赛题为一个NLP与多模态的分类赛题，整体建模采用特征提取、特征交互、预测分类三个阶段**\n",
    "\n",
    "**特征提取：** 对于图像数据，使用ResNet模型进行特征提取、对于文本数据，使用预训练模型Ernie-m多语言模型对中文和英文同时处理，qCap,qImg,（需要验证的标题或图像材料）、caps,imgs（支持验证的文本、图像证据材料）\n",
    "\n",
    "**特征交互**：使用多头自注意力机制，将标题与文本证据材料交互、图像与图像证据材料交互，输出与需要验证的标题和图像的相关证据特征caps_feature、imgs_features\n",
    "\n",
    "**预测分类：** 最后使用全连接层将标题特征、图像特征、相关的文本证据特征、相关的图像证据特征拼接输入到分类器得到最终结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f29e3f853b9445fbeb24189103cdbbcb8364498dc484593a891839994dadbd6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.907532Z",
     "iopub.status.idle": "2023-08-01T15:18:21.907845Z",
     "shell.execute_reply": "2023-08-01T15:18:21.907702Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.907689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode           \n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.resnet = EncoderCNN()\n",
    "        self.classifier1 = nn.Linear(2*(768+2048),1024)\n",
    "        self.classifier2 = nn.Linear(1024,3)\n",
    "        self.attention_text = nn.MultiHeadAttention(768,16)\n",
    "        self.attention_image = nn.MultiHeadAttention(2048,16)\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768,3)\n",
    "        self.resnet.eval()\n",
    "\n",
    "    def forward(self,qCap,qImg,caps,imgs):\n",
    "        self.resnet.eval()\n",
    "        encode_dict_qcap = self.tokenizer(text = qCap ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "        input_ids_qcap = encode_dict_qcap['input_ids']\n",
    "        input_ids_qcap = paddle.to_tensor(input_ids_qcap)\n",
    "        print(input_ids_qcap.shape)\n",
    "        qcap_feature, pooled_output= self.ernie(input_ids_qcap) #(b,length,dim)\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:,0,:].squeeze(1))\n",
    "            return logits\n",
    "        caps_feature = []\n",
    "        for i,caption in enumerate (caps):\n",
    "            encode_dict_cap = self.tokenizer(text = caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "            input_ids_caps = encode_dict_cap['input_ids']\n",
    "            input_ids_caps = paddle.to_tensor(input_ids_caps)\n",
    "            cap_feature, pooled_output= self.ernie(input_ids_caps) #(b,length,dim)\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature,axis=0) #(b,num,length,dim)\n",
    "        caps_feature = caps_feature.mean(axis=1)#(b,length,dim)\n",
    "        caps_feature = self.attention_text(qcap_feature,caps_feature,caps_feature) #(b,length,dim)\n",
    "        imgs_features = []\n",
    "        for img in imgs:\n",
    "            imgs_feature = self.resnet(img) #(length,dim)\n",
    "            imgs_features.append(imgs_feature)\n",
    "        imgs_features = paddle.stack(imgs_features,axis=0) #(b,length,dim)\n",
    "        qImg_features = []\n",
    "        for qImage in qImg:\n",
    "            qImg_feature = self.resnet(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "            qImg_features.append(qImg_feature)\n",
    "        qImg_feature = paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "        imgs_features = self.attention_image(qImg_feature,imgs_features,imgs_features) #(b,1,dim)\n",
    "        # [1, 128, 768] [1, 128, 768] [1, 1, 2048] [1, 1, 2048] origin\n",
    "        # print(qcap_feature.shape,caps_feature.shape,qImg_feature.shape,imgs_features.shape)\n",
    "        # print((qcap_feature[:,0,:].shape,caps_feature[:,0,:].shape,qImg_feature.squeeze(1).shape,imgs_features.squeeze(1).shape))\n",
    "        # ([1,768], [1 , 768], [1, 2048], [1,  2048])\n",
    "        feature = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:], qImg_feature.squeeze(1), imgs_features.squeeze(1)], axis=-1) \n",
    "        logits = self.classifier1(feature)\n",
    "        logits = self.classifier2(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-03 00:25:31,434] [    INFO]\u001b[0m - Model config ErnieMConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ernie_m\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:26:00,148] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-m-base were not used when initializing ErnieMModel: ['cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing ErnieMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:26:00,149] [ WARNING]\u001b[0m - Some weights of ErnieMModel were not initialized from the model checkpoint at ernie-m-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:00,243] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:00,244] [    INFO]\u001b[0m - Already cached C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\ernie_m.sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:01,129] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-08-03 00:26:01,130] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\h\\.paddlenlp\\models\\ernie-m-base\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = NetWork(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.908684Z",
     "iopub.status.idle": "2023-08-01T15:18:21.908993Z",
     "shell.execute_reply": "2023-08-01T15:18:21.908847Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.908834Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-03 00:48:57,790] [    INFO]\u001b[0m - Model config ErnieMConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ernie_m\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:49:27,353] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-m-base were not used when initializing ErnieMModel: ['cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing ErnieMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-08-03 00:49:27,356] [ WARNING]\u001b[0m - Some weights of ErnieMModel were not initialized from the model checkpoint at ernie-m-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<VarType.FP32: 5>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m         hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 使用示例：\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 假设你的模型命名为'model'，输入大小为(batch_size, input_dim)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mcustom_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mernie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 30\u001b[0m, in \u001b[0;36mcustom_summary\u001b[1;34m(model, input_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[0;32m     33\u001b[0m     hook\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1254\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1246\u001b[0m     (\u001b[38;5;129;01mnot\u001b[39;00m in_declarative_mode())\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m in_profiler_mode())\n\u001b[0;32m   1252\u001b[0m ):\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dygraph_call_func(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniconda3\\envs\\Paddle\\lib\\site-packages\\paddlenlp\\transformers\\ernie_m\\modeling.py:282\u001b[0m, in \u001b[0;36mErnieMModel.forward\u001b[1;34m(self, input_ids, position_ids, attention_mask, inputs_embeds, past_key_values, use_cache, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    277\u001b[0m     past_key_values_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# TODO(linjieccc): fix attention mask after uie-m related models updated\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m paddle\u001b[38;5;241m.\u001b[39munsqueeze(\n\u001b[1;32m--> 282\u001b[0m         \u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e4\u001b[39m, axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<VarType.FP32: 5>' as a data type"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def custom_summary(model, input_size):\n",
    "    def get_parameter_count(module):\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "    input_tensor = np.random.randn(*input_size)\n",
    "\n",
    "    def summary_hook(module, input, output):\n",
    "        module_type = type(module).__name__\n",
    "        num_params = get_parameter_count(module)\n",
    "        print(f\"{module_type:<30} | 输入大小: {str(input[0].shape):<25} \"\n",
    "              f\"| 输出大小: {str(output.shape):<25} | 参数数量: {num_params}\")\n",
    "\n",
    "    hooks = []\n",
    "    for layer in model.children():\n",
    "        try:\n",
    "            hook = layer.register_forward_hook(summary_hook)\n",
    "            hooks.append(hook)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    model(input_tensor)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "# 使用示例：\n",
    "# 假设你的模型命名为'model'，输入大小为(batch_size, input_dim)\n",
    "custom_summary(ernie, input_size=(1, 128))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.910269Z",
     "iopub.status.idle": "2023-08-01T15:18:21.910583Z",
     "shell.execute_reply": "2023-08-01T15:18:21.910439Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.910427Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11184 1118\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "print(num_training_steps,warmup_steps)\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-6, num_training_steps, warmup_steps)\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# 创建保存的文件夹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.911666Z",
     "iopub.status.idle": "2023-08-01T15:18:21.911994Z",
     "shell.execute_reply": "2023-08-01T15:18:21.911836Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.911824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义线下评估 评价指标为acc 线上评估是f1score\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:      \n",
    "        labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "        logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.912957Z",
     "iopub.status.idle": "2023-08-01T15:18:21.913262Z",
     "shell.execute_reply": "2023-08-01T15:18:21.913125Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.913111Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train run start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4bb420a45847fcbd1beb7f46a462e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 128]\n",
      "[2, 128]\n",
      "[2, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义训练\n",
    "from tqdm.auto import tqdm\n",
    "def do_train(model, criterion, metric, val_dataloader,train_dataloader):\n",
    "    print(\"train run start\")\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    best_accuracy=0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(tqdm(train_dataloader), start=1):\n",
    "            labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "            probs = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "            loss = criterion(probs, labels)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1 \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # 每间隔一个epoch 在验证集进行评估\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss,eval_accu=evaluate(model, criterion, metric, val_dataloader)\n",
    "                save_param_path = os.path.join(save_dir+str(epoch), 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                if(best_accuracy<eval_accu):\n",
    "                    best_accuracy=eval_accu\n",
    "                    # 保存模型\n",
    "                    save_param_path = os.path.join(best_dir, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "do_train(model, criterion, metric, val_dataloader,train_dataloader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、模型预测\n",
    "**模型预测前，请重启内核，清空占用的显存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.914554Z",
     "iopub.status.idle": "2023-08-01T15:18:21.914930Z",
     "shell.execute_reply": "2023-08-01T15:18:21.914758Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.914738Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint/model_best.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.915828Z",
     "iopub.status.idle": "2023-08-01T15:18:21.916148Z",
     "shell.execute_reply": "2023-08-01T15:18:21.916010Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.915997Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "count=0\n",
    "for batch in test_dataloader:\n",
    "    count+=1\n",
    "    cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "    logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    label = paddle.argmax(probs, axis=1).numpy()\n",
    "    results += label.tolist()\n",
    "    print(count)\n",
    "print(results[:5])\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-01T15:18:21.917503Z",
     "iopub.status.idle": "2023-08-01T15:18:21.917866Z",
     "shell.execute_reply": "2023-08-01T15:18:21.917691Z",
     "shell.execute_reply.started": "2023-08-01T15:18:21.917678Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "import pandas as pd\n",
    "#id/label\n",
    "#字典中的key值即为csv中的列名\n",
    "id_list=range(len(results))\n",
    "print(id_list)\n",
    "frame = pd.DataFrame({'id':id_list,'label':results})\n",
    "frame.to_csv(\"result.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、后续优化\n",
    "\n",
    "baseline分数只有65分，还有很大的改进地方，大家多多尝试，下面是一些想法\n",
    "\n",
    "参数调优：学习率、优化器以及其他超参数等\n",
    "\n",
    "特征提取：更换预训练权重更大的图像特征提取器or文本特征提取器（Ernie or Bert系列）\n",
    "\n",
    "特征交互：目前使用多头自注意力机制对文本与文本证据交互、图像与图像证据交互，可以尝试文本与图像之间的跨模态交互\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
